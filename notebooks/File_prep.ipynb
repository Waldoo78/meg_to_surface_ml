{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import scipy.io as sio\n",
    "import random as rd \n",
    "import pyvista as pv \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.spatial import distance\n",
    "from nilearn.datasets import fetch_surf_fsaverage\n",
    "from nilearn import surface\n",
    "\n",
    "# Utils import\n",
    "from utils.mathutils import compute_vertex_normals, build_template_adjacency_two_hemis, compute_mean_curvature, compute_curvature_differences, compute_hausdorff_metrics, compute_point_distances, compute_normal_differences\n",
    "from utils.file_manip.Matlab_to_array import load_faces, load_vertices\n",
    "from utils.cortical import surface_preprocess as sp\n",
    "from utils.cortical import spherical_harmonics as SH\n",
    "from src.cortical_transformation.reconstruction import reconstruct_brain\n",
    "from utils.mathutils import cart_to_sph\n",
    "from utils.file_manip.vtk_processing import convert_triangles_to_pyvista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=r\"C:\\Users\\wbou2\\Desktop\\meg_to_surface_ml\\src\\cortical_transformation\\data\"\n",
    "main_folder = r\"C:\\Users\\wbou2\\Desktop\\meg_to_surface_ml\\data\\Anatomy_data_CAM_CAN\"\n",
    "template_projection_lh = np.load(os.path.join(data_path, \"lh_sphere_projection.npz\"))\n",
    "template_projection_rh = np.load(os.path.join(data_path, \"rh_sphere_projection.npz\"))\n",
    "\n",
    "\n",
    "sub_name=\"sub-CC110033\"\n",
    "sub_file=os.path.join(main_folder, sub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmax = 80\n",
    "sigma=1e-7\n",
    "lambda_reg=1e-7\n",
    "n_subjects=len(os.listdir(main_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hemisphere-specific harmonics and slice according to lmax (limited to lmax<=80)\n",
    "Y_lh_full = np.load(os.path.join(data_path, \"Y_lh.npz\"))['Y']\n",
    "Y_rh_full = np.load(os.path.join(data_path, \"Y_rh.npz\"))['Y']\n",
    "\n",
    "# Left hemisphere uses first part, right hemisphere uses second part\n",
    "Y_lh = Y_lh_full[:, :(lmax+1)**2]\n",
    "Y_rh = Y_rh_full[:, :(lmax+1)**2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fsaverage\n",
    "fsaverage7 = fetch_surf_fsaverage(mesh='fsaverage7')\n",
    "surf_lh = surface.load_surf_mesh(fsaverage7['pial_left'])\n",
    "surf_rh = surface.load_surf_mesh(fsaverage7['pial_right'])\n",
    "\n",
    "# Output file \n",
    "folder_output = r\"C:\\Users\\wbou2\\Desktop\\meg_to_surface_ml\\data\\fsaverage\"\n",
    "os.makedirs(folder_output, exist_ok=True)\n",
    "\n",
    "# Left Hemi\n",
    "surface_mesh_lh = (surf_lh[0], surf_lh[1])\n",
    "coords_lh, tris_lh = sp.get_resampled_inner_surface(surface_mesh_lh, 'lh')\n",
    "center_lh = np.mean(coords_lh, axis=0)\n",
    "coords_lh = coords_lh - center_lh\n",
    "output_file_lh = os.path.join(folder_output, \"lh_resampled.npz\")\n",
    "np.savez(output_file_lh, coords=coords_lh, tris=tris_lh, center=center_lh)\n",
    "\n",
    "# Right hemi\n",
    "surface_mesh_rh = (surf_rh[0], surf_rh[1])\n",
    "coords_rh, tris_rh = sp.get_resampled_inner_surface(surface_mesh_rh, 'rh')\n",
    "center_rh = np.mean(coords_rh, axis=0)\n",
    "coords_rh = coords_rh - center_rh\n",
    "output_file_rh = os.path.join(folder_output, \"rh_resampled.npz\")\n",
    "np.savez(output_file_rh, coords=coords_rh, tris=tris_rh, center=center_rh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save resampled cortical surfaces\n",
    "for folder in os.listdir(main_folder):\n",
    "    folder_path = os.path.join(main_folder, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Processing subject: {folder}\")\n",
    "        try:\n",
    "            left_vertices_file = os.path.join(folder_path, \"lh_vertices.mat\")\n",
    "            left_faces_file = os.path.join(folder_path, \"lh_faces.mat\")\n",
    "            output_file = os.path.join(folder_path, \"lh_resampled.npz\")\n",
    "            \n",
    "            left_faces = load_faces(left_faces_file)\n",
    "            left_vertices = load_vertices(left_vertices_file)\n",
    "            coords, tris = sp.get_resampled_inner_surface((left_vertices, left_faces), 'lh')\n",
    "            center = np.mean(coords, axis=0)\n",
    "            coords = coords - center\n",
    "            np.savez(output_file, coords=coords, tris=tris, center=center)\n",
    "            print(f\"  Left hemisphere processed successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing left hemisphere: {str(e)}\")\n",
    "        \n",
    "        try:\n",
    "            right_vertices_file = os.path.join(folder_path, \"rh_vertices.mat\")\n",
    "            right_faces_file = os.path.join(folder_path, \"rh_faces.mat\")\n",
    "            output_file = os.path.join(folder_path, \"rh_resampled.npz\")\n",
    "            \n",
    "            right_faces = load_faces(right_faces_file)\n",
    "            right_vertices = load_vertices(right_vertices_file)\n",
    "            coords, tris = sp.get_resampled_inner_surface((right_vertices, right_faces), 'rh')\n",
    "            center = np.mean(coords, axis=0)\n",
    "            coords = coords - center\n",
    "            np.savez(output_file, coords=coords, tris=tris, center=center)\n",
    "            print(f\"  Right hemisphere processed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing right hemisphere: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save coefficients for our template surface \n",
    "\n",
    "fsaverage_path=r\"C:\\Users\\wbou2\\Desktop\\meg_to_surface_ml\\data\\fsaverage\"\n",
    "# Load resampled data\n",
    "fsav_lh = np.load(os.path.join(fsaverage_path, \"lh_resampled.npz\"))\n",
    "fsav_rh = np.load(os.path.join(fsaverage_path, \"rh_resampled.npz\"))\n",
    "\n",
    "# Prepare resampled surfaces for coefficient computation\n",
    "resampled_lh = (fsav_lh['coords'], fsav_lh['tris'])\n",
    "resampled_rh = (fsav_rh['coords'], fsav_rh['tris'])\n",
    "\n",
    "# Compute coefficients for both hemispheres\n",
    "coeffs_fsav_lh = SH.compute_coefficients_SVD(Y_lh, resampled_lh, lmax, lambda_reg=lambda_reg)\n",
    "coeffs_fsav_rh = SH.compute_coefficients_SVD(Y_rh, resampled_rh, lmax, lambda_reg=lambda_reg)\n",
    "\n",
    "# Save coefficients\n",
    "with open(os.path.join(fsaverage_path, \"coeffs_lh.pkl\"), 'wb') as f:\n",
    "    pickle.dump(coeffs_fsav_lh, f)\n",
    "with open(os.path.join(fsaverage_path, \"coeffs_rh.pkl\"), 'wb') as f:\n",
    "    pickle.dump(coeffs_fsav_rh, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save coefficients for each hemisphere of each subject\n",
    "for folder in os.listdir(main_folder):\n",
    "    folder_path = os.path.join(main_folder, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        coeffs_lh_path = os.path.join(folder_path, \"coeffs_lh.pkl\")\n",
    "        coeffs_rh_path = os.path.join(folder_path, \"coeffs_rh.pkl\")\n",
    "        \n",
    "        # Load hemisphere-specific harmonics and slice according to lmax\n",
    "        Y_lh_full = np.load(os.path.join(data_path, \"Y_lh.npz\"))['Y']\n",
    "        Y_rh_full = np.load(os.path.join(data_path, \"Y_rh.npz\"))['Y']\n",
    "        \n",
    "        # Left hemisphere uses first part, right hemisphere uses second part\n",
    "        Y_lh = Y_lh_full[:, :(lmax+1)**2]\n",
    "        Y_rh = Y_rh_full[:, :(lmax+1)**2:]\n",
    "        \n",
    "        print(f\"Processing left hemi of {folder}\")\n",
    "        left_resampled_data = np.load(os.path.join(folder_path, \"lh_resampled.npz\"))\n",
    "        \n",
    "        # Smooth the left hemisphere surface\n",
    "        left_smoothed_coords = sp.smooth_surface(left_resampled_data['coords'], \n",
    "                                           left_resampled_data['tris'],\n",
    "                                           n_iterations=5, \n",
    "                                           relaxation_factor=0.5)\n",
    "        \n",
    "        coeffs_lh = SH.compute_coefficients_SVD(Y_lh, \n",
    "                                              (left_smoothed_coords, left_resampled_data['tris']), \n",
    "                                              lmax, \n",
    "                                              lambda_reg)\n",
    "        with open(coeffs_lh_path, 'wb') as f:\n",
    "            pickle.dump(coeffs_lh, f)\n",
    "        \n",
    "        print(f\"Processing right hemi of {folder}\")\n",
    "        right_resampled_data = np.load(os.path.join(folder_path, \"rh_resampled.npz\"))\n",
    "        \n",
    "        # Smooth the right hemisphere surface\n",
    "        right_smoothed_coords = sp.smooth_surface(right_resampled_data['coords'], \n",
    "                                            right_resampled_data['tris'],\n",
    "                                            n_iterations=5, \n",
    "                                            relaxation_factor=0.5)\n",
    "        \n",
    "        coeffs_rh = SH.compute_coefficients_SVD(Y_rh,\n",
    "                                              (right_smoothed_coords, right_resampled_data['tris']),\n",
    "                                              lmax, \n",
    "                                              lambda_reg)\n",
    "        with open(coeffs_rh_path, 'wb') as f:\n",
    "            pickle.dump(coeffs_rh, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(main_folder):\n",
    "   folder_path = os.path.join(main_folder, folder)\n",
    "   if os.path.isdir(folder_path):\n",
    "       # Load centers from resampled files\n",
    "       lh_data = np.load(os.path.join(folder_path, \"lh_resampled.npz\"))\n",
    "       rh_data = np.load(os.path.join(folder_path, \"rh_resampled.npz\"))\n",
    "       \n",
    "       # Get centers\n",
    "       lh_center = lh_data['center']\n",
    "       rh_center = rh_data['center']\n",
    "       \n",
    "       # Save centers\n",
    "       np.savez(os.path.join(folder_path, \"lh_center.npz\"), center=lh_center)\n",
    "       np.savez(os.path.join(folder_path, \"rh_center.npz\"), center=rh_center)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
