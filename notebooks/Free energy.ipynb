{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import copy\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import scipy as jsp\n",
    "import mne\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from numpyro.infer.autoguide import AutoNormal\n",
    "from numpyro.infer.initialization import init_to_value\n",
    "import numpyro.optim as optim\n",
    "import time\n",
    "\n",
    "\n",
    "from src.Free_energy.utils import (\n",
    "    compute_leadfield, \n",
    "    temporal_reduction,\n",
    "    check_covariance_properties,\n",
    "    regularize_covariance\n",
    ")\n",
    "from utils.cortical.mesh_decimation import generate_and_save_surfaces\n",
    "from utils.file_manip.vtk_processing import convert_triangles_to_pyvista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path definitions\n",
    "PATHS = {\n",
    "   'main': r\"C:\\Users\\wbou2\\Desktop\\meg_to_surface_ml\\data\\Anatomy_data_CAM_CAN\",\n",
    "   'data': r\"C:\\Users\\wbou2\\Desktop\\meg_to_surface_ml\\src\\cortical_transformation\\data\",\n",
    "   'fsaverage': r\"C:\\Users\\wbou2\\Desktop\\meg_to_surface_ml\\data\\fsaverage\",\n",
    "   'subject': r\"C:\\Users\\wbou2\\Desktop\\meg_to_surface_ml\\data\\Anatomy_data_CAM_CAN\\sub-CC710548\",\n",
    "   'meg_task': r\"C:\\Users\\wbou2\\Desktop\\meg_to_surface_ml\\data\\Meg_CAM_CAN\\sub-CC710548\\meg_task\",\n",
    "   \"empty_room\": r\"C:\\Users\\wbou2\\Desktop\\meg_to_surface_ml\\data\\Meg_CAM_CAN\\sub-CC710548\\emptyroom\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Loading spherical harmonics\n",
    "Y_lh_full = np.load(os.path.join(PATHS['data'], \"Y_lh.npz\"))['Y']\n",
    "Y_rh_full = np.load(os.path.join(PATHS['data'], \"Y_rh.npz\"))['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Loading coefficients directly\n",
    "with open(os.path.join(PATHS['subject'], \"coeffs_lh.pkl\"), 'rb') as f:\n",
    "    coeffs_lh = pickle.load(f)\n",
    "with open(os.path.join(PATHS['subject'], \"coeffs_rh.pkl\"), 'rb') as f:\n",
    "    coeffs_rh = pickle.load(f)\n",
    "\n",
    "# Apply epsilon modification to specific harmonics\n",
    "epsilon = 0\n",
    "\n",
    "coeffs_lh_eps = copy.deepcopy(coeffs_lh)\n",
    "coeffs_rh_eps = copy.deepcopy(coeffs_rh)\n",
    "for l in [16, 17]:\n",
    "    for m in range(2*l+1):\n",
    "        coeffs_lh_eps[\"organized_coeffs\"][l][m] += np.array([epsilon+1j*epsilon, epsilon+1j*epsilon, epsilon+1j*epsilon])\n",
    "        coeffs_rh_eps[\"organized_coeffs\"][l][m] += np.array([epsilon+1j*epsilon, epsilon+1j*epsilon, epsilon+1j*epsilon])\n",
    "\n",
    "# 5. Loading centers\n",
    "lh_center = np.load(os.path.join(PATHS['subject'], \"lh_center.npz\"))['center']\n",
    "rh_center = np.load(os.path.join(PATHS['subject'], \"rh_center.npz\"))['center']\n",
    "\n",
    "# 6. Generating surfaces - Passage direct des coefficients et des centres\n",
    "(lh_verts, lh_faces), (rh_verts, rh_faces) = generate_and_save_surfaces(\n",
    "    Y_lh_full=Y_lh_full,\n",
    "    Y_rh_full=Y_rh_full,\n",
    "    lmax=10,\n",
    "    data_path=PATHS['fsaverage'],\n",
    "    merge=False,\n",
    "    coeffs_lh=coeffs_lh_eps,\n",
    "    coeffs_rh=coeffs_rh_eps,\n",
    "    lh_center=lh_center,\n",
    "    rh_center=rh_center\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the MEG channels file\n",
    "meg_channel_path = os.path.join(PATHS['meg_task'], \"channel_vectorview306_acc1.mat\")\n",
    "\n",
    "\n",
    "leadfield, fwd, transform_matrix = compute_leadfield(\n",
    "   meg_channel_path=meg_channel_path,\n",
    "   lh_vertices=lh_verts,  \n",
    "   lh_faces=lh_faces,\n",
    "   rh_vertices=rh_verts,  \n",
    "   rh_faces=rh_faces\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Chargement des donn√©es MEG\n",
    "meg_data = sio.loadmat(os.path.join(PATHS['meg_task'], \"data_block001\"))\n",
    "noise_data=sio.loadmat(os.path.join(PATHS['empty_room'], \"data_block001\"))\n",
    "data_cov = sio.loadmat(os.path.join(PATHS['meg_task'], \"ndatacov_full.mat\"))\n",
    "noise_cov = sio.loadmat(os.path.join(PATHS['meg_task'], \"noisecov_full.mat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_raw=meg_data[\"F\"][:306, 30000:]\n",
    "Y_noise = noise_data['F'][:306,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_reduced, P_full, var_ratio, n_modes = temporal_reduction(\n",
    "    Y_raw[:, :15000],\n",
    "    freq_range=(8, 30),         \n",
    "    variance_explained=0.9,             \n",
    "    sfreq=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(Y, leadfield, Qe, n_components=30):\n",
    "    \"\"\"\n",
    "    Prepare data using SVD dimensionality reduction.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert to jnp arrays (float32)\n",
    "    Y = jnp.asarray(Y, dtype=jnp.float32)\n",
    "    L = jnp.asarray(leadfield, dtype=jnp.float32)\n",
    "    Qe = jnp.asarray(Qe, dtype=jnp.float32)\n",
    "    \n",
    "    # SVD dimensionality reduction\n",
    "    U, s, Vh = jnp.linalg.svd(L, full_matrices=False)\n",
    "    n_components = min(n_components, len(s))\n",
    "    U_k = U[:, :n_components]\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    Y_reduced_spatial = U_k.T @ Y           \n",
    "    L_reduced_spatial = U_k.T @ L\n",
    "    Qe_reduced = jnp.eye(n_components)  \n",
    "    Y_obs = Y_reduced_spatial.T\n",
    "    \n",
    "    # Calculate regularization matrices\n",
    "    M_reg = L_reduced_spatial.T @ L_reduced_spatial\n",
    "    Qa = jnp.linalg.inv(M_reg)\n",
    "    \n",
    "    # Calculate covariance term\n",
    "    cov_term = L_reduced_spatial @ Qa @ L_reduced_spatial.T\n",
    "    \n",
    "    print(f\"Total processing time: {time.time() - start_time:.4f} s\")\n",
    "    \n",
    "    return {\n",
    "        'Y_reduced_spatial': Y_reduced_spatial,\n",
    "        'L_reduced_spatial': L_reduced_spatial,\n",
    "        'Qa': Qa,\n",
    "        'Y_obs': Y_obs,\n",
    "        'Qe_reduced': Qe_reduced,\n",
    "        'cov_term': cov_term,\n",
    "        'U_k': U_k,\n",
    "        'leadfield': L  # Save original matrix\n",
    "    }\n",
    "\n",
    "\n",
    "def model_qe(Y_obs, cov_term, Qe_reduced, Nt, n_components):\n",
    "    \"\"\"\n",
    "    Probabilistic model using pre-calculated and regularized covariance.\n",
    "    Uses two parameters: gamma for noise and beta for sources.\n",
    "    \"\"\"\n",
    "    log_gamma = numpyro.sample(\"log_gamma\", dist.Normal(-5.0, 20.0))\n",
    "    log_beta = numpyro.sample(\"log_beta\", dist.Normal(-5.0, 20.0))\n",
    "    \n",
    "    gamma = jnp.exp(log_gamma)\n",
    "    beta = jnp.exp(log_beta)\n",
    "    \n",
    "    # Combine parameters in covariance model\n",
    "    Sigma = gamma * Qe_reduced + beta * cov_term\n",
    "    mean = jnp.zeros(n_components)\n",
    "    \n",
    "    with numpyro.plate(\"obs\", Nt):\n",
    "        numpyro.sample(\"Y\", dist.MultivariateNormal(mean, covariance_matrix=Sigma), obs=Y_obs)\n",
    "\n",
    "\n",
    "def compute_source_estimate(data_dict, gamma, beta):\n",
    "    \"\"\"\n",
    "    Calculate source estimate J from preprocessed data, gamma and beta.\n",
    "    \"\"\"\n",
    "    L = data_dict['L_reduced_spatial']\n",
    "    Y = data_dict['Y_reduced_spatial']\n",
    "    Qa = beta * data_dict['Qa']  # Scale with beta\n",
    "    Qe = gamma * data_dict['Qe_reduced']\n",
    "    \n",
    "    # Calculate inverse term\n",
    "    inv_term = jnp.linalg.inv(L @ Qa @ L.T + Qe)\n",
    "    \n",
    "    # Calculate gain matrix\n",
    "    K = Qa @ L.T @ inv_term\n",
    "    \n",
    "    # Estimate sources\n",
    "    J = K @ Y\n",
    "    \n",
    "    return J, K\n",
    "\n",
    "\n",
    "def run_inference_qe(Y_reduced, leadfield, Qe, n_components=20, num_steps=200, learning_rate=0.3):\n",
    "    \"\"\"\n",
    "    Run Bayesian inference with Adam optimizer and learning rate schedule.\n",
    "    \"\"\"\n",
    "    global_start = time.time()\n",
    "    print(\"Starting inference...\")\n",
    "    \n",
    "    # Process data for model\n",
    "    data_dict = process_data(Y_reduced, leadfield, Qe, n_components=n_components)\n",
    "    Nt = data_dict['Y_obs'].shape[0]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    init_values = {\n",
    "        \"log_gamma\": 0.0,\n",
    "        \"log_beta\": 4.0\n",
    "    }\n",
    "    guide = AutoNormal(model_qe, init_loc_fn=init_to_value(values=init_values))\n",
    "    \n",
    "    # Setup optimizer with learning rate schedule\n",
    "    scheduler = lambda step: max(learning_rate * (0.95 ** (step // 20)), 1e-4)\n",
    "    optimizer = optim.Adam(scheduler)\n",
    "    \n",
    "    # Initialize SVI\n",
    "    svi = SVI(model_qe, guide, optimizer, loss=Trace_ELBO())\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    svi_state = svi.init(rng_key, data_dict['Y_obs'], data_dict['cov_term'], \n",
    "                        data_dict['Qe_reduced'], Nt, n_components)\n",
    "    \n",
    "    losses = []\n",
    "    params_history = []\n",
    "    \n",
    "    # Run optimization\n",
    "    print(f\"Running {num_steps} optimization steps...\")\n",
    "    for step in range(num_steps):\n",
    "        svi_state, loss = svi.update(\n",
    "            svi_state, data_dict['Y_obs'], data_dict['cov_term'],\n",
    "            data_dict['Qe_reduced'], Nt, n_components\n",
    "        )\n",
    "        losses.append(-loss)\n",
    "        \n",
    "        curr_params = svi.get_params(svi_state)\n",
    "        params_history.append({k: v.copy() for k, v in curr_params.items()})\n",
    "        \n",
    "        if (step + 1) % 20 == 0:\n",
    "            curr_lr = scheduler(step)\n",
    "            curr_values = guide.median(curr_params)\n",
    "            gamma_val = jnp.exp(curr_values['log_gamma'])\n",
    "            beta_val = jnp.exp(curr_values['log_beta'])\n",
    "            print(f'Step {step+1}: ELBO = {-loss:.4f}, Œ≥ = {gamma_val:.4e}, Œ≤ = {beta_val:.4e}')\n",
    "    \n",
    "    # Get final parameters\n",
    "    final_params = svi.get_params(svi_state)\n",
    "    \n",
    "    # Sample from posterior\n",
    "    print(\"Sampling from posterior distribution...\")\n",
    "    num_samples = 1000\n",
    "    rng_key = jax.random.PRNGKey(0)\n",
    "    guide_samples = guide.sample_posterior(rng_key, final_params, sample_shape=(num_samples,))\n",
    "    \n",
    "    gamma_samples = jnp.exp(guide_samples['log_gamma'])\n",
    "    beta_samples = jnp.exp(guide_samples['log_beta'])\n",
    "    \n",
    "    mean_gamma = jnp.mean(gamma_samples)\n",
    "    mean_beta = jnp.mean(beta_samples)\n",
    "    \n",
    "    # Calculate source estimate\n",
    "    print(\"Computing final source estimate...\")\n",
    "    J, K = compute_source_estimate(data_dict, mean_gamma, mean_beta)\n",
    "    \n",
    "    print(f\"Total inference time: {time.time() - global_start:.2f} s\")\n",
    "    \n",
    "    return {\n",
    "        \"params\": final_params,\n",
    "        \"elbo\": losses[-1],\n",
    "        \"gamma\": {\n",
    "            \"mean\": mean_gamma,\n",
    "            \"ci\": jnp.percentile(gamma_samples, jnp.array([2.5, 97.5])),\n",
    "            \"samples\": gamma_samples\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"mean\": mean_beta,\n",
    "            \"ci\": jnp.percentile(beta_samples, jnp.array([2.5, 97.5])),\n",
    "            \"samples\": beta_samples\n",
    "        },\n",
    "        \"losses\": losses,\n",
    "        \"params_history\": params_history,\n",
    "        \"J\": J,\n",
    "        \"data_dict\": data_dict\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the noise covariance matrix\n",
    "Qe = noise_cov['NoiseCov'][:306, :306]  # Extract the first 306 channels (MEG sensors)\n",
    "\n",
    "# Check matrix properties before regularization\n",
    "check_covariance_properties(Qe)\n",
    "\n",
    "# Regularize the noise covariance matrix\n",
    "Qe_reg = regularize_covariance(Qe, percentile=50)\n",
    "\n",
    "# Check properties after regularization\n",
    "check_covariance_properties(Qe_reg)\n",
    "\n",
    "# Run inference on a subset of data\n",
    "Y = Y_raw[:, :40000]\n",
    "result = run_inference_qe(Y, leadfield, Qe_reg, n_components=30, num_steps=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
